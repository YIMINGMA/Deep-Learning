{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Julia and Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Vectorization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorization** is the art of getting rid of explicit ```for``` loops in code.\n",
    "\n",
    "In the deep learning era, you often find yourself training on relatively large datasets, because that is when deep learning algorithms tend to shine. So, it is important that your code runs very quickly, and the ability to perform vectorization has become a key skill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, we have \n",
    "\n",
    "$$\n",
    "z = w^T x + b,\n",
    "$$\n",
    "\n",
    "where $w, x \\in \\mathbb{R}^{n_x}$.\n",
    "\n",
    "To computec $z$, a non vectorized way is:\n",
    "\n",
    "```julia\n",
    "z = 0\n",
    "for i in 1: n_x:\n",
    "    z += w[i] * x[i]\n",
    "z = z .+ b\n",
    "```\n",
    "\n",
    "which can be very slow. In contrast, the vectorized version\n",
    "\n",
    "```julia\n",
    "using LinearAlgebra\n",
    "\n",
    "z = dot(w, x) .+ b\n",
    "```\n",
    "\n",
    "is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  297.942 μs (0 allocations: 0 bytes)\n",
      "  956.823 μs (0 allocations: 0 bytes)\n"
     ]
    }
   ],
   "source": [
    "using Random\n",
    "using LinearAlgebra\n",
    "using BenchmarkTools\n",
    "\n",
    "len = 1000000\n",
    "rng1 = MersenneTwister(1234)\n",
    "a = rand(rng1, len)\n",
    "rng2 = MersenneTwister(12345)\n",
    "b = rand(rng2, len)\n",
    "\n",
    "\n",
    "function inner_product(a, b)\n",
    "    result = 0\n",
    "    for i in 1: length(a) \n",
    "        result += a[i] * b[i] \n",
    "    end\n",
    "    return result\n",
    "end\n",
    "\n",
    "\n",
    "@btime dot($a, $b)\n",
    "@btime inner_product($a, $b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Does Julia Need Vectorization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorization is used to speed up the code without using loop. Using such a function can help in minimizing the running time of code efficiently. There are two meanings of the word vectorization in the high-level languages, and they refer to different things.\n",
    "When we talk about vectorized code in Python/Numpy/MATLAB etc., we are usually referring to the fact that code be like:\n",
    "\n",
    "```python\n",
    "x = [1, 2, 3] \n",
    "y = x + 1\n",
    "```\n",
    "\n",
    "is faster than:\n",
    "\n",
    "```python\n",
    "\n",
    "x = [1, 2, 3] \n",
    "for i in 1:3  \n",
    "  y[i] = x[i] + 1\n",
    "end \n",
    "```\n",
    "\n",
    "The kind of vectorization in the first code block is quite helpful in languages like Python and MATLAB because *generally, every operation in these languages tends to be slow*. Each iteration involves calling ```+``` operator, making array lookups, type-conversions etc. and repeating this iteration for a given number of times makes the overall computation slow. So, it's faster to vectorize the code and only paying the cost of looking up the ```+``` operation once for the entire vector ```x``` rather than once for each element ```x[i]```.\n",
    "\n",
    "We don't come across such problems in Julia, where\n",
    "\n",
    "```julia\n",
    "y .= x .+ 1\n",
    "```\n",
    "\n",
    "and \n",
    "\n",
    "```julia\n",
    "for i in 1:3\n",
    "    y[i] = x[i] + 1\n",
    "end \n",
    "```\n",
    "\n",
    "both compile down to almost the same code and perform comparably. So the kind of vectorization needed in Python and MATLAB is not necessary in Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  355.558 μs (1 allocation: 32 bytes)\n",
      "  499.476 μs (0 allocations: 0 bytes)\n"
     ]
    }
   ],
   "source": [
    "len = 1000000\n",
    "rng = MersenneTwister(1234)\n",
    "a = rand(rng, len)\n",
    "\n",
    "\n",
    "function vectorized(a)\n",
    "    b .= a .+ 1\n",
    "    return b\n",
    "end \n",
    "\n",
    "b = zeros(length(a))\n",
    "function nonvectorized!(a, b)\n",
    "    for i in 1: length(a)\n",
    "        b[i] = a[i] + 1\n",
    "    end\n",
    "    return b\n",
    "end\n",
    "\n",
    "\n",
    "@btime vectorized($a)\n",
    "@btime nonvectorized!($a, $b);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, although vectorization in this case is actually faster than non-vectorization, it is not that advantageous as in the Python case.\n",
    "\n",
    "In fact, *each vectorized operation ends up generating a new temporary array and executing a separate loop, which leads to a lot of overhead when multiple vectorized operations are combined.* So in some cases, we may observe vectorized code to run slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other kind of vectorization pertains to improving performance by using **SIMD (Single Instruction, Multiple Data)** instructions and refers to the CPU's ability to operate on chunks of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Single Instruction, Multiple Data(SIMD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows a simple sum function (returning the sum of all the elements in an array ```arr```):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mysum (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mysum(arr::Vector) \n",
    "    total = zero(eltype(arr)) \n",
    "    for x in arr \n",
    "        total += x \n",
    "    end \n",
    "    return total \n",
    "end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This operation can be generally visualized as serial addition of ```total``` with an array element in every iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     83.535 μs (0.00% GC)\n",
       "  median time:      108.406 μs (0.00% GC)\n",
       "  mean time:        117.555 μs (0.00% GC)\n",
       "  maximum time:     476.823 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate random data \n",
    "rng = MersenneTwister(123)\n",
    "arr = rand(rng, 10^5)\n",
    "@benchmark mysum($arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ```@simd``` can speed up the ```for``` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     11.240 μs (0.00% GC)\n",
       "  median time:      14.703 μs (0.00% GC)\n",
       "  mean time:        17.539 μs (0.00% GC)\n",
       "  maximum time:     143.648 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function mysum1(arr::Vector) \n",
    "    total = zero(eltype(arr)) \n",
    "    @simd for x in arr \n",
    "        total += x \n",
    "    end \n",
    "    return total \n",
    "end \n",
    "  \n",
    "# benchmark the mysum1 function \n",
    "@benchmark mysum1($arr) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the performance increase after using the SIMD macro. So what's actually happening ? By taking advantage of the SIMD instruction set (inbuilt feature of most Intel® CPU's), the add operation is performed in two steps:\n",
    "\n",
    "1. During the first step, intermediate values are accumulated $n$ at a time ($n$ depends on the CPU hardware).\n",
    "2. In a so called **reduction step**, the final $n$ elements are summed together.\n",
    "\n",
    "Now the question arises, how can we combine SIMD vectorization and the language's vectorization capabilities to derive more performance both from the language's compiler and the CPU? In Julia we answer it with the ```LoopVectorization.jl``` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Loop Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**```LoopVectorization.jl``` is a Julia package that provides macros and functions that combine SIMD vectorization and loop reordering so as to improve performance.**\n",
    "\n",
    "```julia\n",
    "@avx macro\n",
    "```\n",
    "\n",
    "It annotates a ```for``` loop, or a set of nested ```for``` loops whose *bounds are constant* across iterations, to optimize the computation.\n",
    "\n",
    "Let's consider the classical dot product problem. To know more about dot product and it's vectorized implementation in python, check out this [article](https://www.geeksforgeeks.org/vectorization-in-python/). In the below examples we will benchmark the same code with different types of vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dotProd_avx (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without using any macro \n",
    "function dotProd(x, y) \n",
    "    prod = zero(eltype(x)) \n",
    "    for i in eachindex(x, y) \n",
    "        prod += x[i] * y[i] \n",
    "    end \n",
    "    prod \n",
    "end \n",
    "\n",
    "# using the simd macro \n",
    "function dotProd_simd(x, y) \n",
    "    prod = zero(eltype(x)) \n",
    "    @simd for i in eachindex(x, y) \n",
    "        prod += x[i] * y[i] \n",
    "    end \n",
    "    prod \n",
    "end \n",
    "\n",
    "# using the avx macro \n",
    "using LoopVectorization \n",
    "function dotProd_avx(x, y) \n",
    "    prod = zero(eltype(x)) \n",
    "    @avx for i in eachindex(x, y) \n",
    "        prod += x[i] * y[i] \n",
    "    end \n",
    "    prod \n",
    "end \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     80.212 μs (0.00% GC)\n",
       "  median time:      90.004 μs (0.00% GC)\n",
       "  mean time:        104.487 μs (0.00% GC)\n",
       "  maximum time:     457.943 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generating random data \n",
    "rng = MersenneTwister(123)\n",
    "x = rand(rng, 10^5); \n",
    "y = rand(rng, 10^5); \n",
    "  \n",
    "# benchmark the function without any macro \n",
    "@benchmark dotProd($x, $y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     83.556 μs (0.00% GC)\n",
       "  median time:      106.612 μs (0.00% GC)\n",
       "  mean time:        133.663 μs (0.00% GC)\n",
       "  maximum time:     1.063 ms (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# benchmark the function with simd macro \n",
    "@benchmark dotProd_simd($x, $y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BenchmarkTools.Trial: \n",
       "  memory estimate:  0 bytes\n",
       "  allocs estimate:  0\n",
       "  --------------\n",
       "  minimum time:     21.046 μs (0.00% GC)\n",
       "  median time:      27.362 μs (0.00% GC)\n",
       "  mean time:        30.701 μs (0.00% GC)\n",
       "  maximum time:     262.207 μs (0.00% GC)\n",
       "  --------------\n",
       "  samples:          10000\n",
       "  evals/sample:     1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# benchmark the function with avx macro \n",
    "@benchmark dotProd_avx($x, $y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that the ```@avx``` macro turns out to have the best performance! The time gap will generally increase for larger sizes of $x$ and $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Julia versus Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia is a *compiled* language, while Python is an *interpreted* language. In some cases Python interpreter can provide fast execution speed, but in other cases it currently is not able to do it (but it does not mean that in the future it will not improve). In particular vectorized functions (like ```dot``` in Julia or ```np.dot``` in Python) are most likely written in some compiled language, so Julia and Python will not differ much in typical cases as they just call this *compiled* function. *However, when you use loops (non-vectorized code) then currently Python will be slower than Julia.* In Julia there is no performance penalty, as long as you write type stable code and use ```a@vx``` (or ```@simd``` and ```@inbounds``` instead) where required. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
