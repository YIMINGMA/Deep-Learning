{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What is Vectorization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorization** is the art of getting rid of explicit ```for``` loops in code.\n",
    "\n",
    "In the deep learning era, you often find yourself training on relatively large datasets, because that is when deep learning algorithms tend to shine. So, it is important that your code runs very quickly, and the ability to perform vectorization has become a key skill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, we have \n",
    "\n",
    "$$\n",
    "z = w^T x + b,\n",
    "$$\n",
    "\n",
    "where $w, x \\in \\mathbb{R}^{n_x}$.\n",
    "\n",
    "To computec $z$, a non vectorized way is:\n",
    "\n",
    "```python\n",
    "z = 0\n",
    "for i in range(n_x):\n",
    "    z += w[i] * x[i]\n",
    "z += b\n",
    "```\n",
    "\n",
    "which can be very slow. In contrast, the vectorized version\n",
    "\n",
    "```python\n",
    "z = np.dot(w, x) + b\n",
    "``` \n",
    "\n",
    "is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized version: 24.733304977416992ms\n",
      "249928.1877257526\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "length = 1000000\n",
    "a = np.random.RandomState(1234).rand(length) # set local seed 1234\n",
    "b = np.random.RandomState(12345).rand(length) # set local seed 12345\n",
    "tic = time.time()\n",
    "c = np.dot(a, b)\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Vectorized version: \" + str(1000*(toc-tic)) + \"ms\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For loop: 376.6160011291504ms\n",
      "249928.18772574305\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range(length):\n",
    "    c += a[i] * b[i]\n",
    "toc = time.time()\n",
    "\n",
    "print(\"For loop: \" + str(1000*(toc-tic)) + \"ms\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Why Vectorization Makes Code Faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of scalable deep learning implementations are done on a GPU or a CPU, both of which have parallelization instructions (**SIMD: Single Instruction Multiple Data**). So, if built-in functions like ```np.dot()``` are used, Python will be enabled to tkae much better advantage of parellelism to do computations much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. More Vectorization Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Whenever possile, avoid explicit ```for``` loops.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute $u = A v$, where $v \\in \\mathbb{R}^{n}$ and $A \\in \\mathbb{R}^{m \\times n}$, the non-vectorized version is\n",
    "\n",
    "```python\n",
    "u = np.zeros((m, 1))\n",
    "for i in range(m):\n",
    "    for j in range(n):\n",
    "        u[i] += A[i][j] * v[j]\n",
    "```\n",
    "\n",
    "while the vectorized version is\n",
    "\n",
    "```python\n",
    "u = np.dot(A, v)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Exponentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say you need to apply the exponential operation on every element of a matrix/vector.\n",
    "\n",
    "Suppose \n",
    "\n",
    "$$\n",
    "v = \\begin{bmatrix}\n",
    "v_1 \\\\\n",
    "v_2 \\\\\n",
    "\\vdots \\\\\n",
    "v_n\n",
    "\\end{bmatrix}\n",
    "\\quad\n",
    "\\text{and}\n",
    "\\quad\n",
    "u = \\begin{bmatrix}\n",
    "e^{v_1} \\\\ \n",
    "e^{v_2} \\\\ \n",
    "\\vdots \\\\ \n",
    "e^{v_n}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "To implement this elementwise exponential operation, the non-vectorized way is \n",
    "\n",
    "```python\n",
    "u = np.zeros((n,1))\n",
    "for i in range(n):\n",
    "    u[i] = math.exp(v[i])\n",
    "```\n",
    "\n",
    "while the vectorized way is \n",
    "\n",
    "```python\n",
    "u = np.exp(v)\n",
    "```\n",
    "\n",
    "There are some other functions similar to ```np.exp```, such as\n",
    "- ```np.log``` performs elementwise $\\log$;\n",
    "- ```np.abs``` performs elementwise taking absolute values;\n",
    "- ```np.maximum``` compares two arrays and returns a new array containing the element-wise maxima;\n",
    "- ```v ** 2``` takes elementwise square ```v```;\n",
    "- ```1 / v``` takes elementwise inverse of ```v```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Logistic Regression Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original logistic regression algorithm is \n",
    "\n",
    "```python\n",
    "J = 0; dw1 = 0; dw2 = 0; db = 0;\n",
    "for i in range(m):\n",
    "    z_i = np.dot(w.T, X[:,i]) + b\n",
    "    a_i = sigmoid(z_i) # sigmoid is a user-defined sigmoid function\n",
    "    J += -( Y[i] * np.log(a_i) + (1-Y[i]) * np.log(1-a_i) )\n",
    "    dz_i = a_i - Y[i]\n",
    "    dw1 += X[1,i] * dz_i \n",
    "    dw2 += X[2,i] * dz_i\n",
    "    db += dz_i\n",
    "J = J/m; dw1 = dw1/m; dw2 = dw2/m; db = db/m;\n",
    "```\n",
    "\n",
    "in which the snippet \n",
    "\n",
    "```python\n",
    "dw1 += X[1,i] * dz_i \n",
    "dw2 += X[2,i] * dz_i\n",
    "```\n",
    "\n",
    "can be replaced with a ```for``` loop over all $n_x$ features in a general:\n",
    "\n",
    "```python\n",
    "for j in range(n_x):\n",
    "    dw[j] += X[j,i] * dz_i # dw = np.zeros((n_x, 1))\n",
    "```\n",
    "\n",
    "Notice that this snippet can be vectorized into\n",
    "\n",
    "```python\n",
    "# initialzation: dw = np.zeros((n_x, 1))\n",
    "dw += X[:,i] * dz_i\n",
    "# dw = dw/m after the loop over all training examples\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vectorizing Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Vectorizing Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In forward propagation, for each training example $(x^{(i)}, y^{(i)})$, we need to compute\n",
    "\n",
    "\\begin{align*}\n",
    "z^{(i)} = & w^T x^{(i)} + b \\\\\n",
    "a^{(i)} = & \\sigma(z^{(i)})\n",
    "\\end{align*}\n",
    "\n",
    "Recall that we defined the design matrix $X$ as\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "| & | & & | \\\\ \n",
    "x^{(1)} & x^{(2)} & \\cdots & x^{(m)} \\\\ \n",
    "| & | & & | \n",
    "\\end{bmatrix}_{n_x \\times m}\n",
    "$$\n",
    "\n",
    "So if we define $Z$ as \n",
    "\n",
    "$$\n",
    "Z = \\begin{bmatrix}\n",
    "z^{(1)} & \n",
    "z^{(2)} &\n",
    "\\cdots &\n",
    "z^{(m)}\n",
    "\\end{bmatrix}_{1 \\times m}\n",
    "$$\n",
    "\n",
    "then \n",
    "\n",
    "$$\n",
    "Z = \\begin{bmatrix}\n",
    "w^T z^{(1)} + b &\n",
    "w^T z^{(2)} + b &\n",
    "\\cdots &\n",
    "w^T z^{(m)} + b \n",
    "\\end{bmatrix} = w^T X + \n",
    "b \\begin{bmatrix}\n",
    "1 & 1 & \\cdots & 1 \n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The corresponding Python code is\n",
    "\n",
    "```python\n",
    "Z = np.dot(w.T, X) + b\n",
    "```\n",
    "\n",
    "in which broadcasting of ```b``` will be used.\n",
    "\n",
    "Let \n",
    "\n",
    "$$\n",
    "A = [ a^{(1)} \\quad a^{(2)} \\quad \\cdots \\quad a^{(m)} ]\n",
    "$$ \n",
    "and we can need to define a vectorized ```sigmoid``` so that ```A = sigmoid(Z)``` can be used to compute ```A```.\n",
    "\n",
    "```python\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Vectorizing Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In backward propagation, for each training example,\n",
    "\n",
    "$$\n",
    "dz^{(i)} = a^{(i)} - y^{(i)}\n",
    "$$\n",
    "\n",
    "Let \n",
    "\n",
    "$$\n",
    "dZ = \\begin{bmatrix}\n",
    "dz^{(1)} & dz^{(2)} & \\cdots & dz^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "y^{(1)} & y^{(2)} & \\cdots & y^{(m)}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then \n",
    "\n",
    "$$\n",
    "dZ = A - Y\n",
    "$$\n",
    "\n",
    "As for $dw \\to dw + x^{(i)} dz^{(i)}$ and $db \\to db + dz^{(i)}$ in each iteration, we can write $dw$ and $db$ into \n",
    "\n",
    "$$\n",
    "dw = \\frac{1}{m} X (dZ)^T\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "db = \\frac{1}{m} \\sum_{i=1}^m dz^{(i)}\n",
    "$$\n",
    "\n",
    "In Python, the corresponding code is \n",
    "\n",
    "```python\n",
    "dw = np.dot(X, dZ.T) / m\n",
    "db = np.sum(dZ) / m\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorized Python code of gradient descent in one step is\n",
    "\n",
    "```python\n",
    "Z = np.dot(w.T, X) + b\n",
    "A = sigmoid(Z)\n",
    "dZ = A - Y\n",
    "dw = np.dot(X, dZ.T) / m\n",
    "db = np.sum(dZ)\n",
    "\n",
    "w = w - alpha * dw\n",
    "b = b - alpha * db\n",
    "```\n",
    "\n",
    "And the full version is\n",
    "\n",
    "```python\n",
    "w = np.zeros((n_x, 1))\n",
    "b = 0\n",
    "counter = 0\n",
    "while counter <= max_iter:\n",
    "    Z = np.dot(w.T, X) + b\n",
    "    A = sigmoid(Z)\n",
    "    dZ = A - Y\n",
    "    dw = np.dot(X, dZ.T) / m\n",
    "    db = np.sum(dZ)\n",
    "    \n",
    "    w = w - alpha * dw\n",
    "    b = b - alpha * db\n",
    "    counter = counter+1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
