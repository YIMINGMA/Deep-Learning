{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Vectorization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorization** is the art of getting rid of explicit ```for``` loops in code.\n",
    "\n",
    "In the deep learning era, you often find yourself training on relatively large datasets, because that is when deep learning algorithms tend to shine. So, it is important that your code runs very quickly, and the ability to perform vectorization has become a key skill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, we have \n",
    "\n",
    "$$\n",
    "z = w^T x + b,\n",
    "$$\n",
    "\n",
    "where $w, x \\in \\mathbb{R}^{n_x}$.\n",
    "\n",
    "To computec $z$, a non vectorized way is:\n",
    "\n",
    "```python\n",
    "z = 0\n",
    "for i in range(n_x):\n",
    "    z += w[i] * x[i]\n",
    "z += b\n",
    "```\n",
    "\n",
    "which can be very slow. In contrast, the vectorized version\n",
    "\n",
    "```python\n",
    "z = np.dot(w, x) + b\n",
    "``` \n",
    "\n",
    "is much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized version: 24.733304977416992ms\n",
      "249928.1877257526\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "length = 1000000\n",
    "a = np.random.RandomState(1234).rand(length) # set local seed 1234\n",
    "b = np.random.RandomState(12345).rand(length) # set local seed 12345\n",
    "tic = time.time()\n",
    "c = np.dot(a, b)\n",
    "toc = time.time()\n",
    "\n",
    "print(\"Vectorized version: \" + str(1000*(toc-tic)) + \"ms\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For loop: 376.6160011291504ms\n",
      "249928.18772574305\n"
     ]
    }
   ],
   "source": [
    "c = 0\n",
    "tic = time.time()\n",
    "for i in range(length):\n",
    "    c += a[i] * b[i]\n",
    "toc = time.time()\n",
    "\n",
    "print(\"For loop: \" + str(1000*(toc-tic)) + \"ms\")\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Vectorization Makes Code Faster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of scalable deep learning implementations are done on a GPU or a CPU, both of which have parallelization instructions (**SIMD: Single Instruction Multiple Data**). So, if built-in functions like ```np.dot()``` are used, Python will be enabled to tkae much better advantage of parellelism to do computations much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
